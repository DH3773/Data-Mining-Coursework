{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3204,"status":"ok","timestamp":1686270550187,"user":{"displayName":"Alex Thomo","userId":"08504196803322236588"},"user_tz":420},"id":"mcUDHs_wjnJa"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import cross_val_score\n","\n","# suppress warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":250},"executionInfo":{"elapsed":3866,"status":"ok","timestamp":1686270597754,"user":{"displayName":"Alex Thomo","userId":"08504196803322236588"},"user_tz":420},"id":"XiCZNEvBjnJi","outputId":"caaa09b0-9d75-41bc-c60b-4c5df55a10e6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword location                                               text  \\\n","0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n","1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n","2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n","3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n","4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n","\n","   target  \n","0       1  \n","1       1  \n","2       1  \n","3       1  \n","4       1  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# read the data\n","df = pd.read_csv('https://raw.githubusercontent.com/nikjohn7/Disaster-Tweets-Kaggle/main/data/train.csv')\n","df.head()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1686270606497,"user":{"displayName":"Alex Thomo","userId":"08504196803322236588"},"user_tz":420},"id":"xyuDgpX5jnJm","outputId":"adeb073a-dad9-462e-ea88-4e128637751d"},"outputs":[{"data":{"text/plain":["(7613, 5)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# how many rows and columns are in the data set?\n","df.shape"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":206,"status":"ok","timestamp":1686270709316,"user":{"displayName":"Alex Thomo","userId":"08504196803322236588"},"user_tz":420},"id":"nryA0ie5jnJs","outputId":"a1a370ff-b62f-4447-e8d2-3fc532ce8c22"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\godpi\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download('stopwords')\n","\n","stopwords = set(nltk.corpus.stopwords.words('english'))\n","\n","\n","#I was thinking about including all of the location attribute values as stop words, \n","#but I decided against it, as often distasters like wildfires are centered\n","#around a specific location, and I don't want to lose that information.\n","\n","#However, it should be noted that the model trained as is will attribute distaster or\n","#non-disaster weight to certain locations due to the presence of those locations and the content\n","#of the tweets in the training data. This is a limitation of the model."]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":453,"status":"ok","timestamp":1686270868485,"user":{"displayName":"Alex Thomo","userId":"08504196803322236588"},"user_tz":420},"id":"8jB0xwj_jnJt","outputId":"a2ef7a57-d78b-46c1-cb28-4452b3e7ad4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","    Disaster       0.80      0.89      0.84       886\n","Non-Disaster       0.82      0.68      0.75       637\n","\n","    accuracy                           0.80      1523\n","   macro avg       0.81      0.79      0.79      1523\n","weighted avg       0.81      0.80      0.80      1523\n","\n"]}],"source":["# build a text processing and classifier pipeline\n","# to predict whether a tweet is about a real disaster or not\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn import svm\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import classification_report\n","\n","\n","df2 = df.copy()\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(df2['text'], df2['target'], test_size=0.2)\n","\n","# Create a pipeline that first transforms the text data into TF-IDF vectors, then applies SVM\n","text_clf = Pipeline([\n","    ('tfidf', TfidfVectorizer(stop_words=list(stopwords))),\n","    ('clf', svm.SVC(random_state=np.random.seed(1))),\n","])\n","\n","# Train the classifier\n","text_clf.fit(X_train, y_train)\n","\n","# Predict the test set results\n","y_pred = text_clf.predict(X_test)\n","\n","# Print the classification report\n","print(classification_report(y_test, y_pred, target_names=['Disaster', 'Non-Disaster']))\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2119,"status":"ok","timestamp":1686270971052,"user":{"displayName":"Alex Thomo","userId":"08504196803322236588"},"user_tz":420},"id":"htvOCTKkjnJw"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>0.100000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>-0.018750</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7608</th>\n","      <td>10869</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Two giant cranes holding a bridge collapse int...</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7609</th>\n","      <td>10870</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n","      <td>1</td>\n","      <td>0.150000</td>\n","    </tr>\n","    <tr>\n","      <th>7610</th>\n","      <td>10871</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n","      <td>1</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7611</th>\n","      <td>10872</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Police investigating after an e-bike collided ...</td>\n","      <td>1</td>\n","      <td>-0.260417</td>\n","    </tr>\n","    <tr>\n","      <th>7612</th>\n","      <td>10873</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>The Latest: More Homes Razed by Northern Calif...</td>\n","      <td>1</td>\n","      <td>0.500000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7613 rows × 6 columns</p>\n","</div>"],"text/plain":["         id keyword location  \\\n","0         1     NaN      NaN   \n","1         4     NaN      NaN   \n","2         5     NaN      NaN   \n","3         6     NaN      NaN   \n","4         7     NaN      NaN   \n","...     ...     ...      ...   \n","7608  10869     NaN      NaN   \n","7609  10870     NaN      NaN   \n","7610  10871     NaN      NaN   \n","7611  10872     NaN      NaN   \n","7612  10873     NaN      NaN   \n","\n","                                                   text  target  sentiment  \n","0     Our Deeds are the Reason of this #earthquake M...       1   0.000000  \n","1                Forest fire near La Ronge Sask. Canada       1   0.100000  \n","2     All residents asked to 'shelter in place' are ...       1  -0.018750  \n","3     13,000 people receive #wildfires evacuation or...       1   0.000000  \n","4     Just got sent this photo from Ruby #Alaska as ...       1   0.000000  \n","...                                                 ...     ...        ...  \n","7608  Two giant cranes holding a bridge collapse int...       1   0.000000  \n","7609  @aria_ahrary @TheTawniest The out of control w...       1   0.150000  \n","7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   0.000000  \n","7611  Police investigating after an e-bike collided ...       1  -0.260417  \n","7612  The Latest: More Homes Razed by Northern Calif...       1   0.500000  \n","\n","[7613 rows x 6 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# This script creates a new column 'sentiment' in the dataframe, \n","# which contains the sentiment score of the text. \n","# The sentiment score is a float within the range [-1.0, 1.0], \n","# where -1.0 denotes a very negative sentiment, \n","# 1.0 denotes a very positive sentiment, \n","# and values around 0 denote a neutral sentiment.\n","\n","from textblob import TextBlob\n","\n","# Define a function to apply sentiment analysis to a text\n","def get_sentiment(text):\n","    blob = TextBlob(text)\n","    return blob.sentiment.polarity  # returns a value between -1 and 1\n","\n","# Create a new column 'sentiment' in the DataFrame\n","df2['sentiment'] = df2['text'].apply(get_sentiment)\n","\n","# Display the DataFrame\n","df2"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":209,"status":"ok","timestamp":1686270975562,"user":{"displayName":"Alex Thomo","userId":"08504196803322236588"},"user_tz":420},"id":"Rkq_X6gDjnJz","outputId":"81d72884-1a98-4df9-a99d-e41ed14eb116"},"outputs":[{"data":{"text/plain":["target\n","0    0.070622\n","1    0.018631\n","Name: sentiment, dtype: float64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# find average sentiment for each tweet in each class in df2\n","df2.groupby('target')['sentiment'].mean()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1686271000768,"user":{"displayName":"Alex Thomo","userId":"08504196803322236588"},"user_tz":420},"id":"3tqgyAaijnJ0","outputId":"70150a23-bdce-454a-a747-c2ef7e62e942"},"outputs":[{"data":{"text/plain":["keyword\n","hazardous               0.457891\n","razed                   0.418946\n","outbreak                0.312661\n","mayhem                  0.277262\n","wreckage                0.273440\n","                          ...   \n","trapped                -0.160049\n","structural%20failure   -0.195099\n","airplane%20accident    -0.202232\n","violent%20storm        -0.510888\n","bloody                 -0.522698\n","Name: sentiment, Length: 221, dtype: float64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# find average sentiment for each keyword in df2\n","# order the results from most positive to most negative\n","\n","df2.groupby('keyword')['sentiment'].mean().sort_values(ascending=False)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","    Disaster       0.77      0.89      0.83       846\n","Non-Disaster       0.83      0.66      0.74       677\n","\n","    accuracy                           0.79      1523\n","   macro avg       0.80      0.78      0.78      1523\n","weighted avg       0.80      0.79      0.79      1523\n","\n"]}],"source":["# Out of curiousity, I want to build a classifier that uses the sentiment of a tweet along with\n","# the text to predict whether a tweet is about a real disaster or not.\n","\n","\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import FunctionTransformer\n","\n","# Define a function to select the 'sentiment' attribute\n","def select_sentiment(X):\n","    return X[['sentiment']].values\n","\n","# Create a pipeline that first transforms the 'text' attribute into TF-IDF vectors, and leaves\n","# the 'sentiment' attribute as is, then applies SVM\n","tweet_pipeline = Pipeline([\n","    ('features', ColumnTransformer([\n","        ('text', TfidfVectorizer(stop_words=list(stopwords)), 'text'),\n","        ('sentiment', FunctionTransformer(select_sentiment, validate=False), ['sentiment']),\n","    ])),\n","    ('clf', svm.SVC()),\n","])\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(df2[['text', 'sentiment']], df2['target'], test_size=0.2)\n","\n","# Train the classifier\n","tweet_pipeline.fit(X_train, y_train)\n","\n","# Predict the test set results\n","y_pred = tweet_pipeline.predict(X_test)\n","\n","# Print the classification report\n","print(classification_report(y_test, y_pred, target_names=['Disaster', 'Non-Disaster']))\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, the impact is negligable, which likely implies that the SVM likely figures out the relationships between the words and the sentiment on its own, without the help of TextBlob"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
